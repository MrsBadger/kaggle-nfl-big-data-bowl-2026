{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5d18e5",
   "metadata": {},
   "source": [
    "MODEL USED : lightgbm \n",
    "DATASET : ALL WEEKS (input_2023_w01 to w18 / output_2023_w01 to w18)\n",
    "OUTPUT FRAMES PREDICTED : 15 \n",
    "INPUT FRAMES NEEDED BEFORE THROW : 8 (X-7, X-6, ..., X-1, X0) \n",
    "METHOD : SEQUENTIAL TRAINING [(...X0, X1) ---> (...X1, X2) ---> ETC] \n",
    "SEPARATE MODELS FOR X AND Y : YES \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load ALL input datasets (w01 to w18)\n",
    "print(\"Loading all input datasets (w01 to w18)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "input_dfs = []\n",
    "for week in range(1, 19):\n",
    "    filename = f'train/input_2023_w{week:02d}.csv'\n",
    "    print(f\"Loading {filename}...\", end=\" \")\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"✓ Shape: {df.shape}\")\n",
    "    input_dfs.append(df)\n",
    "\n",
    "# Concatenate all input dataframes\n",
    "df_input = pd.concat(input_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ All input datasets loaded and merged!\")\n",
    "print(f\"  Total shape: {df_input.shape}\")\n",
    "print(f\"  Unique games: {df_input['game_id'].nunique()}\")\n",
    "print(f\"  Unique players: {df_input['nfl_id'].nunique()}\")\n",
    "\n",
    "# Load ALL output datasets (w01 to w18)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading all output datasets (w01 to w18)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dfs = []\n",
    "for week in range(1, 19):\n",
    "    filename = f'train/output_2023_w{week:02d}.csv'\n",
    "    print(f\"Loading {filename}...\", end=\" \")\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"✓ Shape: {df.shape}\")\n",
    "    output_dfs.append(df)\n",
    "\n",
    "# Concatenate all output dataframes\n",
    "df_output = pd.concat(output_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ All output datasets loaded and merged!\")\n",
    "print(f\"  Total shape: {df_output.shape}\")\n",
    "print(f\"  Unique games: {df_output['game_id'].nunique()}\")\n",
    "print(f\"  Unique players: {df_output['nfl_id'].nunique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA LOADING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"df_input:  {df_input.shape}\")\n",
    "print(f\"df_output: {df_output.shape}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of df_input:\")\n",
    "df_input.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_TRUE = df_input[df_input['player_to_predict'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57053da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check what's the lowest number of available input frames\n",
    "\n",
    "\n",
    "s = df_input_TRUE['frame_id'] \n",
    "\n",
    "# Identify starts of new sequences: whenever the value decreases\n",
    "groups = (s < s.shift()).cumsum()\n",
    "\n",
    "# Compute the maximum in each group (i.e., the \"last\" value)\n",
    "last_vals = s.groupby(groups).max()\n",
    "\n",
    "# Get the smallest last value\n",
    "result = last_vals.min()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_TRUE = df_input_TRUE.drop(columns=[\"player_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ef691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Keep ALL players, but for each player, keep only their last 8 input frames\n",
    "print(f\"Before filtering - df_input_TRUE shape: {df_input_TRUE.shape}\")\n",
    "print(f\"Before filtering - df_output shape: {df_output.shape}\")\n",
    "print(f\"num_frames_output distribution:\\n{df_input_TRUE['num_frames_output'].value_counts().sort_index()}\")\n",
    "\n",
    "# Get the last 8 frame_ids for each player (sorted descending, take top 8)\n",
    "last_8_frames = df_input_TRUE.groupby(['game_id', 'play_id', 'nfl_id']).apply(\n",
    "    lambda x: x.nlargest(8, 'frame_id')[['game_id', 'play_id', 'nfl_id', 'frame_id']],
    include_groups=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nLast 8 frames shape: {last_8_frames.shape}\")\n",
    "print(f\"Frames per player:\\n{last_8_frames.groupby(['game_id', 'play_id', 'nfl_id']).size().value_counts()}\")\n",
    "\n",
    "# Keep only rows that match the last 3 frames for each player\n",
    "df_input_TRUE = df_input_TRUE.merge(\n",
    "    last_8_frames, \n",
    "    on=['game_id', 'play_id', 'nfl_id', 'frame_id'], \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After keeping only last 3 input frames per player: {df_input_TRUE.shape}\")\n",
    "\n",
    "# Step 2: For df_output, keep ALL players but cap output frames at 15\n",
    "# If a player has more than 15 output frames, keep only frames 1-15\n",
    "print(f\"\\n--- Processing df_output ---\")\n",
    "print(f\"df_output frame_id range: {df_output['frame_id'].min()} to {df_output['frame_id'].max()}\")\n",
    "\n",
    "# Keep only output frames 1 through 15\n",
    "df_output = df_output[df_output['frame_id'] <= 15].copy()\n",
    "\n",
    "print(f\"After capping output frames at 15: {df_output.shape}\")\n",
    "print(f\"df_output frame_id range after filtering: {df_output['frame_id'].min()} to {df_output['frame_id'].max()}\")\n",
    "\n",
    "# Verify we have the same players in both datasets\n",
    "players_in_input = set(df_input_TRUE[['game_id', 'play_id', 'nfl_id']].apply(tuple, axis=1))\n",
    "players_in_output = set(df_output[['game_id', 'play_id', 'nfl_id']].apply(tuple, axis=1))\n",
    "\n",
    "print(f\"\\nPlayers in df_input_TRUE: {len(players_in_input)}\")\n",
    "print(f\"Players in df_output: {len(players_in_output)}\")\n",
    "print(f\"Players in both: {len(players_in_input.intersection(players_in_output))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20914849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frame numbering where 0 is the last frame, -1 is second last, -2 is third last\n",
    "# Sort by frame_id within each player to ensure proper ordering\n",
    "df_input_TRUE = df_input_TRUE.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "df_input_TRUE['frame_position'] = df_input_TRUE.groupby(['game_id', 'play_id', 'nfl_id']).cumcount() - 2\n",
    "\n",
    "print(f\"Frame positions per player:\\n{df_input_TRUE['frame_position'].value_counts().sort_index()}\")\n",
    "print(f\"\\nSample data with frame_position (note: -2=oldest, -1=middle, 0=newest):\")\n",
    "print(df_input_TRUE[['game_id', 'play_id', 'nfl_id', 'frame_id', 'frame_position', 'x', 'y']].head(10))\n",
    "\n",
    "# Pivot df_input_TRUE to wide format - one observation per player\n",
    "# Grouping identifiers (everything except the variables to pivot)\n",
    "id_vars = ['game_id', 'play_id', 'player_to_predict', 'nfl_id', \n",
    "           'play_direction', 'absolute_yardline_number', \n",
    "           'player_height', 'player_weight', 'player_birth_date', \n",
    "           'player_position', 'player_side', 'player_role', \n",
    "           'num_frames_output', 'ball_land_x', 'ball_land_y']\n",
    "\n",
    "# Create the pivot using frame_position instead of frame_id\n",
    "df_wide = df_input_TRUE.pivot_table(\n",
    "    index=id_vars,\n",
    "    columns='frame_position',\n",
    "    values=['x', 'y', 's', 'a', 'dir', 'o'],\n",
    "    aggfunc='first'  # Use first in case of duplicates\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "# This creates column names like: x_-2, x_-1, x_0, y_-2, y_-1, y_0, etc.\n",
    "# Where 0 = last frame, -1 = second last, -2 = third last\n",
    "df_wide.columns = ['_'.join([str(c) for c in col]).strip('_') if col[1] != '' else col[0] \n",
    "                   for col in df_wide.columns.values]\n",
    "\n",
    "print(f\"\\nOriginal shape: {df_input_TRUE.shape}\")\n",
    "print(f\"Wide format shape: {df_wide.shape}\")\n",
    "print(f\"\\nAll columns: {df_wide.columns.tolist()}\")\n",
    "df_wide.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d12895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot df_output to wide format\n",
    "print(f\"df_output shape before pivot: {df_output.shape}\")\n",
    "print(f\"df_output columns: {df_output.columns.tolist()}\")\n",
    "print(f\"\\nSample df_output:\")\n",
    "print(df_output.head(10))\n",
    "\n",
    "# Pivot output data - x and y for each future frame\n",
    "df_output_wide = df_output.pivot_table(\n",
    "    index=['game_id', 'play_id', 'nfl_id'],\n",
    "    columns='frame_id',\n",
    "    values=['x', 'y'],\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names - this creates x_1, x_2, ..., y_1, y_2, ...\n",
    "df_output_wide.columns = ['_'.join([str(c) for c in col]).strip('_') if col[1] != '' else col[0] \n",
    "                          for col in df_output_wide.columns.values]\n",
    "\n",
    "print(f\"\\ndf_output_wide shape: {df_output_wide.shape}\")\n",
    "print(f\"Output columns: {[col for col in df_output_wide.columns if col.startswith(('x_', 'y_'))]}\")\n",
    "\n",
    "# Merge with df_wide\n",
    "df_combined = df_wide.merge(\n",
    "    df_output_wide,\n",
    "    on=['game_id', 'play_id', 'nfl_id'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nCombined shape: {df_combined.shape}\")\n",
    "print(f\"\\nAll columns: {df_combined.columns.tolist()}\")\n",
    "\n",
    "# Reorder columns to have input features first, then outputs\n",
    "# Get base columns (non-pivoted)\n",
    "base_cols = ['game_id', 'play_id', 'player_to_predict', 'nfl_id', \n",
    "             'play_direction', 'absolute_yardline_number', \n",
    "             'player_height', 'player_weight', 'player_birth_date', \n",
    "             'player_position', 'player_side', 'player_role', \n",
    "             'num_frames_output', 'ball_land_x', 'ball_land_y']\n",
    "\n",
    "# Get input frame columns (x_-2, x_-1, x_0, y_-2, y_-1, y_0, etc.)\n",
    "input_cols = [col for col in df_combined.columns \n",
    "              if any(col.endswith(f'_{i}') for i in [-2, -1, 0])]\n",
    "\n",
    "# Get output frame columns (x_1, x_2, ..., y_1, y_2, ...)\n",
    "output_cols = [col for col in df_combined.columns \n",
    "               if col.startswith(('x_', 'y_')) and col not in input_cols]\n",
    "\n",
    "# Sort to have proper order: x cols for inputs, then x cols for outputs, then y cols\n",
    "x_input = sorted([col for col in input_cols if col.startswith('x_')], \n",
    "                 key=lambda x: int(x.split('_')[1]))\n",
    "y_input = sorted([col for col in input_cols if col.startswith('y_')], \n",
    "                 key=lambda x: int(x.split('_')[1]))\n",
    "other_input = [col for col in input_cols if not col.startswith(('x_', 'y_'))]\n",
    "\n",
    "x_output = sorted([col for col in output_cols if col.startswith('x_')], \n",
    "                  key=lambda x: int(x.split('_')[1]))\n",
    "y_output = sorted([col for col in output_cols if col.startswith('y_')], \n",
    "                  key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "# Reorder: base + other_input + x_input + x_output + y_input + y_output\n",
    "df_combined = df_combined[base_cols + other_input + x_input + x_output + y_input + y_output]\n",
    "\n",
    "print(f\"\\nReordered columns: {df_combined.columns.tolist()}\")\n",
    "print(f\"\\nFinal combined dataframe:\")\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize movement direction - make all plays go in the same direction (right)\n",
    "print(f\"Play direction distribution before standardization:\")\n",
    "print(df_combined['play_direction'].value_counts())\n",
    "\n",
    "# Identify which columns need to be flipped for \"left\" plays\n",
    "x_cols = [col for col in df_combined.columns if col.startswith('x_')]\n",
    "y_cols = [col for col in df_combined.columns if col.startswith('y_')]\n",
    "dir_cols = [col for col in df_combined.columns if col.startswith('dir_')]\n",
    "o_cols = [col for col in df_combined.columns if col.startswith('o_')]\n",
    "\n",
    "print(f\"\\nColumns to standardize:\")\n",
    "print(f\"x columns: {x_cols}\")\n",
    "print(f\"y columns: {y_cols}\")\n",
    "print(f\"dir columns: {dir_cols}\")\n",
    "print(f\"o columns: {o_cols}\")\n",
    "\n",
    "# Create a mask for left plays\n",
    "left_mask = df_combined['play_direction'] == 'left'\n",
    "print(f\"\\nNumber of 'left' plays: {left_mask.sum()}\")\n",
    "print(f\"Number of 'right' plays: {(~left_mask).sum()}\")\n",
    "\n",
    "# For left plays, flip the coordinates:\n",
    "# - x: flip horizontally (120 - x, assuming field is 120 yards)\n",
    "# - y: flip vertically (53.3 - y, assuming field is 53.3 yards wide)\n",
    "# - dir: add 180 degrees and modulo 360\n",
    "# - o: add 180 degrees and modulo 360\n",
    "# - s, a: keep unchanged (magnitudes don't change direction)\n",
    "\n",
    "# Check sample values before transformation\n",
    "print(f\"\\nSample values BEFORE standardization (left play):\")\n",
    "sample_left = df_combined[left_mask].iloc[0]\n",
    "print(f\"play_direction: {sample_left['play_direction']}\")\n",
    "print(f\"x_0: {sample_left['x_0']}, y_0: {sample_left['y_0']}\")\n",
    "if 'x_1' in df_combined.columns:\n",
    "    print(f\"x_1: {sample_left['x_1']}, y_1: {sample_left['y_1']}\")\n",
    "print(f\"dir_0: {sample_left['dir_0']}, o_0: {sample_left['o_0']}\")\n",
    "\n",
    "# Flip x coordinates for left plays (120 - x)\n",
    "for col in x_cols:\n",
    "    df_combined.loc[left_mask, col] = 120 - df_combined.loc[left_mask, col]\n",
    "\n",
    "# Flip y coordinates for left plays (53.3 - y)\n",
    "for col in y_cols:\n",
    "    df_combined.loc[left_mask, col] = 53.3 - df_combined.loc[left_mask, col]\n",
    "\n",
    "# Flip direction angles for left plays (add 180 and modulo 360)\n",
    "for col in dir_cols:\n",
    "    df_combined.loc[left_mask, col] = (df_combined.loc[left_mask, col] + 180) % 360\n",
    "\n",
    "# Flip orientation angles for left plays (add 180 and modulo 360)\n",
    "for col in o_cols:\n",
    "    df_combined.loc[left_mask, col] = (df_combined.loc[left_mask, col] + 180) % 360\n",
    "\n",
    "# Also flip ball_land_x and ball_land_y\n",
    "df_combined.loc[left_mask, 'ball_land_x'] = 120 - df_combined.loc[left_mask, 'ball_land_x']\n",
    "df_combined.loc[left_mask, 'ball_land_y'] = 53.3 - df_combined.loc[left_mask, 'ball_land_y']\n",
    "\n",
    "# Also flip absolute_yardline_number\n",
    "df_combined.loc[left_mask, 'absolute_yardline_number'] = 120 - df_combined.loc[left_mask, 'absolute_yardline_number']\n",
    "\n",
    "print(f\"\\nSample values AFTER standardization (originally left play):\")\n",
    "sample_left_after = df_combined[left_mask].iloc[0]\n",
    "print(f\"play_direction (original): {sample_left['play_direction']}\")\n",
    "print(f\"x_0: {sample_left_after['x_0']}, y_0: {sample_left_after['y_0']}\")\n",
    "if 'x_1' in df_combined.columns:\n",
    "    print(f\"x_1: {sample_left_after['x_1']}, y_1: {sample_left_after['y_1']}\")\n",
    "print(f\"dir_0: {sample_left_after['dir_0']}, o_0: {sample_left_after['o_0']}\")\n",
    "\n",
    "print(f\"\\nStandardization complete! All plays now oriented in the same direction.\")\n",
    "print(f\"Shape: {df_combined.shape}\")\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert player height from feet-inches to centimeters\n",
    "print(\"Converting player height to centimeters...\")\n",
    "print(f\"Sample heights before conversion: {df_combined['player_height'].head(10).tolist()}\")\n",
    "\n",
    "def height_to_cm(height_str):\n",
    "    \"\"\"Convert height from 'feet-inches' format to centimeters\"\"\"\n",
    "    if pd.isna(height_str):\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        feet, inches = height_str.split('-')\n",
    "        feet = int(feet)\n",
    "        inches = int(inches)\n",
    "        \n",
    "        # Convert to centimeters: 1 foot = 30.48 cm, 1 inch = 2.54 cm\n",
    "        cm = (feet * 30.48) + (inches * 2.54)\n",
    "        return round(cm, 2)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_combined['player_height_cm'] = df_combined['player_height'].apply(height_to_cm)\n",
    "print(f\"\\nSample heights after conversion (cm): {df_combined['player_height_cm'].head(10).tolist()}\")\n",
    "\n",
    "# Drop the original height column\n",
    "df_combined = df_combined.drop(columns=['player_height'])\n",
    "\n",
    "# Convert player birth date to age in years\n",
    "print(\"\\n\\nConverting birth date to age...\")\n",
    "print(f\"Sample birth dates: {df_combined['player_birth_date'].head(10).tolist()}\")\n",
    "\n",
    "# Extract game date from game_id (format: YYYYMMDD##)\n",
    "# game_id like 2023090700 -> year=2023, month=09, day=07\n",
    "def extract_game_date(game_id):\n",
    "    \"\"\"Extract date from game_id\"\"\"\n",
    "    game_id_str = str(game_id)\n",
    "    year = int(game_id_str[0:4])\n",
    "    month = int(game_id_str[4:6])\n",
    "    day = int(game_id_str[6:8])\n",
    "    return pd.Timestamp(year=year, month=month, day=day)\n",
    "\n",
    "# Get game dates\n",
    "df_combined['game_date'] = df_combined['game_id'].apply(extract_game_date)\n",
    "\n",
    "# Convert birth_date to datetime\n",
    "df_combined['player_birth_date'] = pd.to_datetime(df_combined['player_birth_date'])\n",
    "\n",
    "# Calculate age at time of game\n",
    "df_combined['player_age'] = (df_combined['game_date'] - df_combined['player_birth_date']).dt.days / 365.25\n",
    "\n",
    "# Round to 2 decimal places\n",
    "df_combined['player_age'] = df_combined['player_age'].round(2)\n",
    "\n",
    "print(f\"\\nSample ages: {df_combined['player_age'].head(10).tolist()}\")\n",
    "print(f\"Age statistics:\\n{df_combined['player_age'].describe()}\")\n",
    "\n",
    "# Drop temporary columns\n",
    "df_combined = df_combined.drop(columns=['player_birth_date', 'game_date'])\n",
    "\n",
    "print(f\"\\nConversions complete!\")\n",
    "print(f\"Shape: {df_combined.shape}\")\n",
    "print(f\"\\nUpdated columns (first 25): {df_combined.columns[:25].tolist()}\")\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "TRAIN/TEST SPLIT STRATEGY\n",
    "================================================================================\n",
    "\n",
    "CRITICAL: We split by game_id to prevent data leakage\n",
    "- All players from a game stay together (either train or test)\n",
    "- This ensures no information about a test game appears in training\n",
    "\n",
    "SPLIT RATIO: 80/20 (4/5 for training, 1/5 for testing)\n",
    "\n",
    "NEXT STEPS:\n",
    "- df_combined will be split into train and test portions\n",
    "- Train portion → df_ready (with column cleaning)\n",
    "- Test portion → detailed split for proper evaluation (next cell)\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Split data: 1/5 for test, 4/5 for training (based on game_id)\n",
    "print(f\"Original df_combined shape: {df_combined.shape}\")\n",
    "print(f\"Unique game_ids: {df_combined['game_id'].nunique()}\")\n",
    "\n",
    "# Get unique game_ids\n",
    "unique_games = df_combined['game_id'].unique()\n",
    "print(f\"\\nTotal unique games: {len(unique_games)}\")\n",
    "\n",
    "# Sample 1/5 of game_ids for test set\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_test_games = int(len(unique_games) * 0.2)  # 1/5 = 20%\n",
    "test_games = np.random.choice(unique_games, size=n_test_games, replace=False)\n",
    "\n",
    "print(f\"Number of games in test set: {len(test_games)}\")\n",
    "print(f\"Number of games in training set: {len(unique_games) - len(test_games)}\")\n",
    "\n",
    "# Split the data - keep test data with ALL information for now\n",
    "df_test_full = df_combined[df_combined['game_id'].isin(test_games)].copy()\n",
    "df_train_full = df_combined[~df_combined['game_id'].isin(test_games)].copy()\n",
    "\n",
    "# Verify no overlap BEFORE dropping columns\n",
    "assert len(set(df_test_full['game_id']).intersection(set(df_train_full['game_id']))) == 0, \"Error: Overlap between test and training game_ids!\"\n",
    "print(\"\\n✓ Verified: No overlap between test and training game_ids\")\n",
    "print(f\"Test set games: {sorted(df_test_full['game_id'].unique())}\")\n",
    "\n",
    "# For training: remove identifier columns and unnecessary metadata to create df_ready\n",
    "# CRITICAL: game_id, play_id, nfl_id are identifiers, NOT features!\n",
    "# Training a model with these would cause overfitting to specific games/plays/players\n",
    "columns_to_remove = ['game_id', 'play_id', 'nfl_id', 'player_to_predict', \n",
    "                     'play_direction', 'num_frames_output', 'ball_land_x', 'ball_land_y']\n",
    "df_ready = df_train_full.drop(columns=columns_to_remove)\n",
    "\n",
    "print(f\"\\nAfter split and column removal:\")\n",
    "print(f\"df_test_full shape: {df_test_full.shape} (keeps ALL columns for proper evaluation)\")\n",
    "print(f\"df_ready shape: {df_ready.shape} (training data, removed {len(columns_to_remove)} columns)\")\n",
    "\n",
    "# Encode categorical variables for LightGBM\n",
    "print(f\"\\n--- Encoding categorical variables ---\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = ['player_position', 'player_side', 'player_role']\n",
    "\n",
    "# Check what we have\n",
    "print(f\"Checking categorical columns in df_ready:\")\n",
    "for col in categorical_cols:\n",
    "    if col in df_ready.columns:\n",
    "        print(f\"  {col}: dtype={df_ready[col].dtype}, unique values={df_ready[col].nunique()}\")\n",
    "\n",
    "# Store encoders for later use on test data\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_ready.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_ready[col] = le.fit_transform(df_ready[col].astype(str))\n",
    "        encoders[col] = le\n",
    "        print(f\"  ✓ Encoded {col}: {len(le.classes_)} unique categories\")\n",
    "\n",
    "print(f\"\\ndf_ready after encoding:\")\n",
    "print(f\"  Shape: {df_ready.shape}\")\n",
    "print(f\"  Data types:\")\n",
    "print(df_ready.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67587fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PROPER TEST SET PREPARATION FOR MODEL EVALUATION\n",
    "================================================================================\n",
    "\n",
    "THE PROBLEM:\n",
    "When we train sequential models to predict x_1, x_2, ..., x_15 and y_1, ..., y_15,\n",
    "we need to evaluate them properly. But not all players have all 15 output frames!\n",
    "\n",
    "EXAMPLE SCENARIO:\n",
    "- Player A has 3 output frames → We can only evaluate predictions for frames 1-3\n",
    "- Player B has 15 output frames → We can evaluate all predictions for frames 1-15\n",
    "- Player C has 10 output frames → We can only evaluate predictions for frames 1-10\n",
    "\n",
    "THE SOLUTION:\n",
    "We create THREE dataframes from df_test_full:\n",
    "\n",
    "1. df_test_informative:\n",
    "   - Keeps: game_id, play_id, nfl_id, num_frames_output\n",
    "   - Purpose: Track which player has how many actual output frames\n",
    "   - This is ESSENTIAL for meaningful evaluation\n",
    "\n",
    "2. df_test_y_true:\n",
    "   - Keeps: x_1 through x_15, y_1 through y_15 (the actual observed values)\n",
    "   - Purpose: The ground truth for comparing predictions\n",
    "   - NA values indicate frames that were not observed for that player\n",
    "\n",
    "3. df_test_X:\n",
    "   - Keeps: Only input features (x_-2, x_-1, x_0, y_-2, y_-1, y_0, etc.)\n",
    "   - Removes: Output frames AND metadata columns\n",
    "   - Purpose: This is what we feed into the model for prediction\n",
    "\n",
    "EVALUATION WORKFLOW:\n",
    "1. Feed df_test_X into trained models → get predictions\n",
    "2. Compare predictions with df_test_y_true\n",
    "3. Use df_test_informative to know how many frames to compare for each player\n",
    "4. Only evaluate frames that actually exist (using num_frames_output)\n",
    "\n",
    "CRITICAL: The index alignment across all three dataframes allows us to match:\n",
    "- Predictions (from df_test_X)\n",
    "- Ground truth (from df_test_y_true)  \n",
    "- Player metadata (from df_test_informative)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"Preparing test dataset for proper model evaluation...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Start with df_test_full (has all columns including output frames)\n",
    "df_test_informative = df_test_full.copy()\n",
    "\n",
    "print(f\"df_test_full shape: {df_test_full.shape}\")\n",
    "print(f\"Columns in df_test_full: {df_test_full.columns.tolist()}\")\n",
    "\n",
    "# 1. Create df_test_informative: Keep only player identification and num_frames_output\n",
    "info_cols = ['game_id', 'play_id', 'nfl_id', 'num_frames_output']\n",
    "df_test_informative = df_test_full[info_cols].copy()\n",
    "\n",
    "print(f\"\\n1. df_test_informative created:\")\n",
    "print(f\"   Shape: {df_test_informative.shape}\")\n",
    "print(f\"   Columns: {df_test_informative.columns.tolist()}\")\n",
    "print(f\"   Distribution of num_frames_output:\")\n",
    "print(df_test_informative['num_frames_output'].value_counts().sort_index())\n",
    "\n",
    "# 2. Create df_test_y_true: Keep only the output frames (x_1:x_15, y_1:y_15)\n",
    "output_cols = [f'x_{i}' for i in range(1, 16)] + [f'y_{i}' for i in range(1, 16)]\n",
    "# Only keep columns that exist\n",
    "output_cols = [col for col in output_cols if col in df_test_full.columns]\n",
    "df_test_y_true = df_test_full[output_cols].copy()\n",
    "\n",
    "print(f\"\\n2. df_test_y_true created:\")\n",
    "print(f\"   Shape: {df_test_y_true.shape}\")\n",
    "print(f\"   Columns: {df_test_y_true.columns.tolist()}\")\n",
    "print(f\"   NA counts per output frame:\")\n",
    "for col in output_cols[:5]:  # Show first 5 as example\n",
    "    print(f\"   {col}: {df_test_y_true[col].isna().sum()} NAs\")\n",
    "\n",
    "# 3. Create df_test_X: Remove output frames AND metadata columns\n",
    "cols_to_remove = output_cols + ['game_id', 'play_id', 'nfl_id', 'player_to_predict', \n",
    "                                 'play_direction', 'num_frames_output', 'ball_land_x', 'ball_land_y']\n",
    "# Only remove columns that exist\n",
    "cols_to_remove = [col for col in cols_to_remove if col in df_test_full.columns]\n",
    "df_test_X = df_test_full.drop(columns=cols_to_remove)\n",
    "\n",
    "print(f\"\\n3. df_test_X created:\")\n",
    "print(f\"   Shape: {df_test_X.shape}\")\n",
    "print(f\"   Columns: {df_test_X.columns.tolist()}\")\n",
    "print(f\"   Removed {len(cols_to_remove)} columns\")\n",
    "\n",
    "# Encode categorical variables using the same encoders from training\n",
    "print(f\"\\n   Encoding categorical variables in test data...\")\n",
    "for col in categorical_cols:\n",
    "    if col in df_test_X.columns and col in encoders:\n",
    "        # Handle unseen categories by assigning them to -1\n",
    "        df_test_X[col] = df_test_X[col].astype(str)\n",
    "        df_test_X[col] = df_test_X[col].apply(\n",
    "            lambda x: encoders[col].transform([x])[0] if x in encoders[col].classes_ else -1\n",
    "        )\n",
    "        print(f\"      ✓ Encoded {col}\")\n",
    "\n",
    "print(f\"\\n   df_test_X data types after encoding:\")\n",
    "print(df_test_X.dtypes)\n",
    "\n",
    "# Verify index alignment\n",
    "assert len(df_test_informative) == len(df_test_y_true) == len(df_test_X), \"Index mismatch!\"\n",
    "print(f\"\\n✓ Verified: All three dataframes have matching indices ({len(df_test_X)} rows)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET PREPARATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"df_test_informative: {df_test_informative.shape} - Player metadata\")\n",
    "print(f\"df_test_y_true: {df_test_y_true.shape} - Actual output frames (ground truth)\")\n",
    "print(f\"df_test_X: {df_test_X.shape} - Input features for prediction\")\n",
    "print(\"\\n✓ Ready for model evaluation with proper frame-level validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "BLOCK EXPLANATION: SEQUENTIAL PREDICTION SETUP FOR TRAJECTORY FORECASTING\n",
    "================================================================================\n",
    "\n",
    "GOAL: Create 30 separate training datasets for predicting future player positions\n",
    "\n",
    "STRATEGY: Sequential/Autoregressive Prediction\n",
    "- We predict each future position step-by-step\n",
    "- Each prediction uses the previous predictions as additional features\n",
    "- This allows models to learn from their own trajectory progression\n",
    "\n",
    "WHAT THIS BLOCK CREATES:\n",
    "1. For X-coordinates: 15 datasets (df_train_x_1 through df_train_x_15)\n",
    "2. For Y-coordinates: 15 datasets (df_train_y_1 through df_train_y_15)\n",
    "3. Total: 30 training sets + 30 target vectors\n",
    "\n",
    "HOW IT WORKS - Example with X-coordinates:\n",
    "\n",
    "├─ df_train_x_1 (predicting x_1):\n",
    "│  Features: Base features only (x_-2, x_-1, x_0, y_-2, y_-1, y_0, etc.)\n",
    "│  Target: y_x_1 (the actual x_1 value)\n",
    "│  Filters: Only players with valid x_1\n",
    "│\n",
    "├─ df_train_x_2 (predicting x_2):\n",
    "│  Features: Base features + x_1 (predicted in previous step)\n",
    "│  Target: y_x_2 (the actual x_2 value)\n",
    "│  Filters: Only players with valid x_1 AND x_2\n",
    "│\n",
    "├─ df_train_x_3 (predicting x_3):\n",
    "│  Features: Base features + x_1 + x_2 (previous predictions)\n",
    "│  Target: y_x_3 (the actual x_3 value)\n",
    "│  Filters: Only players with valid x_1, x_2, AND x_3\n",
    "│\n",
    "└─ ... continues through x_15\n",
    "\n",
    "KEY INSIGHT:\n",
    "- As we go further into the future (x_1 → x_15), we have MORE features\n",
    "- But we also have FEWER training examples (more NAs in later frames)\n",
    "- Each model builds on predictions from previous steps\n",
    "\n",
    "SAME LOGIC APPLIES TO Y-COORDINATES (df_train_y_1 through df_train_y_15)\n",
    "\n",
    "OUTPUTS:\n",
    "- train_x_dfs: Dictionary with keys 1-15, values are DataFrames\n",
    "- y_x_targets: Dictionary with keys 1-15, values are Series (targets)\n",
    "- train_y_dfs: Dictionary with keys 1-15, values are DataFrames\n",
    "- y_y_targets: Dictionary with keys 1-15, values are Series (targets)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Create all training datasets for x_1 to x_15 and y_1 to y_15\n",
    "print(\"Creating all training datasets systematically...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get base features (everything except x_1:x_15 and y_1:y_15)\n",
    "all_cols = df_ready.columns.tolist()\n",
    "x_output_cols = [f'x_{i}' for i in range(1, 16)]\n",
    "y_output_cols = [f'y_{i}' for i in range(1, 16)]\n",
    "base_features = [col for col in all_cols if col not in x_output_cols + y_output_cols]\n",
    "\n",
    "print(f\"Base features: {base_features}\")\n",
    "print(f\"\\nTotal base features: {len(base_features)}\")\n",
    "\n",
    "# Dictionaries to store all dataframes and targets\n",
    "train_x_dfs = {}\n",
    "train_y_dfs = {}\n",
    "y_x_targets = {}\n",
    "y_y_targets = {}\n",
    "\n",
    "# Create training sets for x_1 to x_15\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating X training sets...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(1, 16):\n",
    "    print(f\"\\nCreating df_train_x_{i} and y_x_{i}...\")\n",
    "    \n",
    "    # Filter: must have valid x_1, x_2, ..., x_i\n",
    "    valid_mask = df_ready[[f'x_{j}' for j in range(1, i+1)]].notna().all(axis=1)\n",
    "    df_valid = df_ready[valid_mask].copy()\n",
    "    \n",
    "    print(f\"  Rows with valid x_1 to x_{i}: {len(df_valid)} / {len(df_ready)}\")\n",
    "    \n",
    "    # Target: x_i\n",
    "    y_x_targets[i] = df_valid[f'x_{i}'].copy()\n",
    "    \n",
    "    # Features: base + x_1 to x_(i-1)\n",
    "    if i == 1:\n",
    "        # For x_1, only use base features\n",
    "        feature_cols = base_features\n",
    "    else:\n",
    "        # For x_i (i>1), use base features + x_1 to x_(i-1)\n",
    "        prev_x_cols = [f'x_{j}' for j in range(1, i)]\n",
    "        feature_cols = base_features + prev_x_cols\n",
    "    \n",
    "    train_x_dfs[i] = df_valid[feature_cols].copy()\n",
    "    \n",
    "    print(f\"  df_train_x_{i} shape: {train_x_dfs[i].shape}\")\n",
    "    print(f\"  y_x_{i} shape: {y_x_targets[i].shape}\")\n",
    "    print(f\"  Features: {len(feature_cols)} ({len(base_features)} base + {i-1} previous x)\")\n",
    "\n",
    "# Create training sets for y_1 to y_15\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Y training sets...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(1, 16):\n",
    "    print(f\"\\nCreating df_train_y_{i} and y_y_{i}...\")\n",
    "    \n",
    "    # Filter: must have valid y_1, y_2, ..., y_i\n",
    "    valid_mask = df_ready[[f'y_{j}' for j in range(1, i+1)]].notna().all(axis=1)\n",
    "    df_valid = df_ready[valid_mask].copy()\n",
    "    \n",
    "    print(f\"  Rows with valid y_1 to y_{i}: {len(df_valid)} / {len(df_ready)}\")\n",
    "    \n",
    "    # Target: y_i\n",
    "    y_y_targets[i] = df_valid[f'y_{i}'].copy()\n",
    "    \n",
    "    # Features: base + y_1 to y_(i-1)\n",
    "    if i == 1:\n",
    "        # For y_1, only use base features\n",
    "        feature_cols = base_features\n",
    "    else:\n",
    "        # For y_i (i>1), use base features + y_1 to y_(i-1)\n",
    "        prev_y_cols = [f'y_{j}' for j in range(1, i)]\n",
    "        feature_cols = base_features + prev_y_cols\n",
    "    \n",
    "    train_y_dfs[i] = df_valid[feature_cols].copy()\n",
    "    \n",
    "    print(f\"  df_train_y_{i} shape: {train_y_dfs[i].shape}\")\n",
    "    print(f\"  y_y_{i} shape: {y_y_targets[i].shape}\")\n",
    "    print(f\"  Features: {len(feature_cols)} ({len(base_features)} base + {i-1} previous y)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY - All datasets created successfully!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nX Training Sets:\")\n",
    "for i in range(1, 16):\n",
    "    print(f\"  df_train_x_{i}: {train_x_dfs[i].shape}, y_x_{i}: {y_x_targets[i].shape}\")\n",
    "\n",
    "print(\"\\nY Training Sets:\")\n",
    "for i in range(1, 16):\n",
    "    print(f\"  df_train_y_{i}: {train_y_dfs[i].shape}, y_y_{i}: {y_y_targets[i].shape}\")\n",
    "\n",
    "print(f\"\\nTotal datasets created: {len(train_x_dfs) + len(train_y_dfs)} training sets\")\n",
    "print(f\"Total target vectors created: {len(y_x_targets) + len(y_y_targets)} targets\")\n",
    "\n",
    "# Update the individual variables for easy access\n",
    "df_train_x_1 = train_x_dfs[1]\n",
    "df_train_y_1 = train_y_dfs[1]\n",
    "y_x_1 = y_x_targets[1]\n",
    "y_y_1 = y_y_targets[1]\n",
    "\n",
    "print(\"\\n✓ Individual variables updated: df_train_x_1, df_train_y_1, y_x_1, y_y_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection of the 4 dictionaries\n",
    "print(\"=\"*80)\n",
    "print(\"INSPECTING THE 4 DICTIONARIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. DICTIONARY STRUCTURE - Overview\n",
    "print(\"\\n1. DICTIONARY STRUCTURE - Keys and Types\")\n",
    "print(\"-\"*80)\n",
    "print(f\"train_x_dfs keys: {list(train_x_dfs.keys())}\")\n",
    "print(f\"train_x_dfs type: {type(train_x_dfs)}\")\n",
    "print(f\"Number of entries: {len(train_x_dfs)}\")\n",
    "\n",
    "print(f\"\\ny_x_targets keys: {list(y_x_targets.keys())}\")\n",
    "print(f\"y_x_targets type: {type(y_x_targets)}\")\n",
    "print(f\"Number of entries: {len(y_x_targets)}\")\n",
    "\n",
    "print(f\"\\ntrain_y_dfs keys: {list(train_y_dfs.keys())}\")\n",
    "print(f\"train_y_dfs type: {type(train_y_dfs)}\")\n",
    "print(f\"Number of entries: {len(train_y_dfs)}\")\n",
    "\n",
    "print(f\"\\ny_y_targets keys: {list(y_y_targets.keys())}\")\n",
    "print(f\"y_y_targets type: {type(y_y_targets)}\")\n",
    "print(f\"Number of entries: {len(y_y_targets)}\")\n",
    "\n",
    "# 2. INSPECT FIRST FEW ENTRIES - X predictions\n",
    "print(\"\\n\\n2. INSPECTING X PREDICTIONS - First 3 Datasets\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATASET {i}: Predicting x_{i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = train_x_dfs[i]\n",
    "    target = y_x_targets[i]\n",
    "    \n",
    "    print(f\"\\ntrain_x_dfs[{i}]:\")\n",
    "    print(f\"  Type: {type(df)}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns ({len(df.columns)}): {df.columns.tolist()}\")\n",
    "    print(f\"\\n  First 3 rows:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    print(f\"\\ny_x_targets[{i}]:\")\n",
    "    print(f\"  Type: {type(target)}\")\n",
    "    print(f\"  Shape: {target.shape}\")\n",
    "    print(f\"  Name: {target.name if hasattr(target, 'name') else 'N/A'}\")\n",
    "    print(f\"  First 5 values: {target.head().tolist()}\")\n",
    "    print(f\"  Statistics: min={target.min():.2f}, max={target.max():.2f}, mean={target.mean():.2f}, std={target.std():.2f}\")\n",
    "    \n",
    "    # Verify index alignment\n",
    "    print(f\"\\n  ✓ Index alignment check: {df.index.equals(target.index)}\")\n",
    "\n",
    "# 3. INSPECT MIDDLE AND LATE ENTRIES - X predictions\n",
    "print(\"\\n\\n3. INSPECTING X PREDICTIONS - Middle (x_7) and Late (x_15) Datasets\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in [7, 15]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATASET {i}: Predicting x_{i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = train_x_dfs[i]\n",
    "    target = y_x_targets[i]\n",
    "    \n",
    "    print(f\"\\ntrain_x_dfs[{i}]:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Number of features: {len(df.columns)}\")\n",
    "    \n",
    "    # Identify which previous x values are included\n",
    "    x_features = [col for col in df.columns if col.startswith('x_')]\n",
    "    print(f\"  Previous x predictions included: {x_features}\")\n",
    "    \n",
    "    print(f\"\\n  Sample row (first row):\")\n",
    "    print(df.iloc[0])\n",
    "    \n",
    "    print(f\"\\ny_x_targets[{i}]:\")\n",
    "    print(f\"  Shape: {target.shape}\")\n",
    "    print(f\"  Statistics: min={target.min():.2f}, max={target.max():.2f}, mean={target.mean():.2f}\")\n",
    "\n",
    "# 4. COMPARE Y PREDICTIONS - Show pattern is the same\n",
    "print(\"\\n\\n4. INSPECTING Y PREDICTIONS - Sample Datasets\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in [1, 5, 10]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATASET {i}: Predicting y_{i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = train_y_dfs[i]\n",
    "    target = y_y_targets[i]\n",
    "    \n",
    "    print(f\"\\ntrain_y_dfs[{i}]:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    \n",
    "    # Identify which previous y values are included\n",
    "    y_features = [col for col in df.columns if col.startswith('y_') and col not in ['y_-2', 'y_-1', 'y_0']]\n",
    "    print(f\"  Previous y predictions included: {y_features if y_features else 'None (only base features)'}\")\n",
    "    \n",
    "    print(f\"\\ny_y_targets[{i}]:\")\n",
    "    print(f\"  Shape: {target.shape}\")\n",
    "    print(f\"  Statistics: min={target.min():.2f}, max={target.max():.2f}, mean={target.mean():.2f}\")\n",
    "\n",
    "# 5. SUMMARY COMPARISON\n",
    "print(\"\\n\\n5. SUMMARY - Feature Count Progression\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nX Predictions Feature Count:\")\n",
    "for i in range(1, 16):\n",
    "    print(f\"  df_train_x_{i}: {train_x_dfs[i].shape[1]} features, {train_x_dfs[i].shape[0]} samples\")\n",
    "\n",
    "print(\"\\nY Predictions Feature Count:\")\n",
    "for i in range(1, 16):\n",
    "    print(f\"  df_train_y_{i}: {train_y_dfs[i].shape[1]} features, {train_y_dfs[i].shape[0]} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSPECTION COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "TRAINING 30 SEQUENTIAL MODELS FOR TRAJECTORY PREDICTION\n",
    "================================================================================\n",
    "\n",
    "TRAINING STRATEGY:\n",
    "1. For each x_i (i=1 to 15):\n",
    "   - Start with train_x_dfs[i] (has base features + x_1 to x_(i-1) placeholders)\n",
    "   - For i > 1: Replace x_(i-1) column with TRUE values from y_x_targets[i-1]\n",
    "   - Train model on augmented features to predict y_x_targets[i]\n",
    "   \n",
    "2. Same process for y_i (i=1 to 15)\n",
    "\n",
    "KEY INSIGHT: We use TRUE previous values during training, but will use\n",
    "PREDICTED values during inference (autoregressive prediction)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"Training 30 sequential models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check data types before training\n",
    "print(\"\\nChecking data types in df_ready...\")\n",
    "print(df_ready.dtypes)\n",
    "\n",
    "# Check for object types (categorical that need encoding)\n",
    "object_cols = df_ready.select_dtypes(include=['object']).columns.tolist()\n",
    "if object_cols:\n",
    "    print(f\"\\n⚠️  WARNING: Found object dtype columns: {object_cols}\")\n",
    "    print(\"These need to be encoded. Please re-run Cell 8 to apply encoding!\")\n",
    "    raise ValueError(f\"Object dtype columns found: {object_cols}. Re-run Cell 8 to encode them.\")\n",
    "\n",
    "print(\"\\n✓ All columns are numeric - ready for training\")\n",
    "\n",
    "# Import LightGBM parameters\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "# Store all trained models\n",
    "models_x = {}\n",
    "models_y = {}\n",
    "\n",
    "# Train X models\n",
    "print(\"\\nTRAINING X MODELS (x_1 to x_15)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(1, 16):\n",
    "    print(f\"\\nTraining model_x_{i}...\")\n",
    "    \n",
    "    # Get training data\n",
    "    X_train = train_x_dfs[i].copy()\n",
    "    y_train = y_x_targets[i].copy()\n",
    "    \n",
    "    # For i > 1, merge TRUE previous target (x_(i-1)) into features\n",
    "    if i > 1:\n",
    "        # The train_x_dfs[i] already has x_1 to x_(i-1) from the filtering\n",
    "        # We just need to verify it's there\n",
    "        prev_cols = [f'x_{j}' for j in range(1, i)]\n",
    "        for col in prev_cols:\n",
    "            if col not in X_train.columns:\n",
    "                print(f\"  ERROR: Missing {col} in training data!\")\n",
    "    \n",
    "    print(f\"  Features: {X_train.shape[1]}, Samples: {X_train.shape[0]}\")\n",
    "    print(f\"  Target: {y_train.shape[0]} values (min={y_train.min():.2f}, max={y_train.max():.2f})\")\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model\n",
    "    models_x[i] = model\n",
    "    \n",
    "    # Calculate training MAE\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    print(f\"  ✓ Training MAE: {train_mae:.4f}\")\n",
    "\n",
    "# Train Y models\n",
    "print(\"\\n\\nTRAINING Y MODELS (y_1 to y_15)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(1, 16):\n",
    "    print(f\"\\nTraining model_y_{i}...\")\n",
    "    \n",
    "    # Get training data\n",
    "    X_train = train_y_dfs[i].copy()\n",
    "    y_train = y_y_targets[i].copy()\n",
    "    \n",
    "    # For i > 1, verify previous y targets are in features\n",
    "    if i > 1:\n",
    "        prev_cols = [f'y_{j}' for j in range(1, i)]\n",
    "        for col in prev_cols:\n",
    "            if col not in X_train.columns:\n",
    "                print(f\"  ERROR: Missing {col} in training data!\")\n",
    "    \n",
    "    print(f\"  Features: {X_train.shape[1]}, Samples: {X_train.shape[0]}\")\n",
    "    print(f\"  Target: {y_train.shape[0]} values (min={y_train.min():.2f}, max={y_train.max():.2f})\")\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model\n",
    "    models_y[i] = model\n",
    "    \n",
    "    # Calculate training MAE\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    print(f\"  ✓ Training MAE: {train_mae:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total models trained: {len(models_x) + len(models_y)}\")\n",
    "print(f\"  X models: {len(models_x)}\")\n",
    "print(f\"  Y models: {len(models_y)}\")\n",
    "print(\"\\n✓ All 30 models ready for sequential prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "SEQUENTIAL TRAJECTORY PREDICTION FUNCTION\n",
    "================================================================================\n",
    "\n",
    "FUNCTION: predict_full_trajectory()\n",
    "\n",
    "INPUT: \n",
    "- DataFrame with base features (including x_-2, x_-1, x_0, y_-2, y_-1, y_0)\n",
    "- Optional ground truth for evaluation (df_true_x, df_true_y)\n",
    "\n",
    "OUTPUT:\n",
    "- DataFrame with all 15 predicted x and y positions\n",
    "- Quality metrics (MAE, RMSE) if ground truth provided\n",
    "- Visualization plots\n",
    "\n",
    "PROCESS:\n",
    "1. Start with input features\n",
    "2. Predict x_1, add to features\n",
    "3. Predict x_2 using x_1 prediction, add to features\n",
    "4. Continue through x_15\n",
    "5. Same process for y_1 through y_15\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "def predict_full_trajectory(df_input, df_true_x=None, df_true_y=None, \n",
    "                           plot_results=True, sample_trajectories=5):\n",
    "    \"\"\"\n",
    "    Predict complete 15-frame trajectory using sequential models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_input : DataFrame\n",
    "        Input features (must contain base features like x_-2, x_-1, x_0, etc.)\n",
    "    df_true_x : DataFrame, optional\n",
    "        Ground truth x positions (columns x_1 to x_15) for evaluation\n",
    "    df_true_y : DataFrame, optional\n",
    "        Ground truth y positions (columns y_1 to y_15) for evaluation\n",
    "    plot_results : bool\n",
    "        Whether to generate visualization plots\n",
    "    sample_trajectories : int\n",
    "        Number of sample trajectories to visualize\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : DataFrame\n",
    "        Predicted x_1 to x_15 and y_1 to y_15\n",
    "    metrics : dict\n",
    "        Evaluation metrics if ground truth provided\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PREDICTING FULL TRAJECTORY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Input samples: {len(df_input)}\")\n",
    "    \n",
    "    # Initialize predictions dataframe\n",
    "    df_pred = df_input.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # PREDICT X POSITIONS (x_1 to x_15)\n",
    "    # ========================================\n",
    "    print(\"\\nPredicting X positions (x_1 to x_15)...\")\n",
    "    \n",
    "    for i in range(1, 16):\n",
    "        # Get the features that model_x[i] was trained on\n",
    "        # model_x[i] was trained on base features + x_1 to x_(i-1)\n",
    "        expected_features = train_x_dfs[i].columns.tolist()\n",
    "        \n",
    "        # Extract only those features from df_pred\n",
    "        X_for_prediction = df_pred[expected_features]\n",
    "        \n",
    "        # Predict x_i\n",
    "        x_pred = models_x[i].predict(X_for_prediction)\n",
    "        \n",
    "        # Add prediction to dataframe for next iteration\n",
    "        df_pred[f'x_{i}'] = x_pred\n",
    "        \n",
    "        if (i % 5 == 0) or (i == 1):\n",
    "            print(f\"  ✓ Predicted x_{i} (min={x_pred.min():.2f}, max={x_pred.max():.2f}, mean={x_pred.mean():.2f})\")\n",
    "    \n",
    "    # ========================================\n",
    "    # PREDICT Y POSITIONS (y_1 to y_15)\n",
    "    # ========================================\n",
    "    print(\"\\nPredicting Y positions (y_1 to y_15)...\")\n",
    "    \n",
    "    for i in range(1, 16):\n",
    "        # Get the features that model_y[i] was trained on\n",
    "        # model_y[i] was trained on base features + y_1 to y_(i-1)\n",
    "        expected_features = train_y_dfs[i].columns.tolist()\n",
    "        \n",
    "        # Extract only those features from df_pred\n",
    "        X_for_prediction = df_pred[expected_features]\n",
    "        \n",
    "        # Predict y_i\n",
    "        y_pred = models_y[i].predict(X_for_prediction)\n",
    "        \n",
    "        # Add prediction to dataframe for next iteration\n",
    "        df_pred[f'y_{i}'] = y_pred\n",
    "        \n",
    "        if (i % 5 == 0) or (i == 1):\n",
    "            print(f\"  ✓ Predicted y_{i} (min={y_pred.min():.2f}, max={y_pred.max():.2f}, mean={y_pred.mean():.2f})\")\n",
    "    \n",
    "    # Extract predictions\n",
    "    x_pred_cols = [f'x_{i}' for i in range(1, 16)]\n",
    "    y_pred_cols = [f'y_{i}' for i in range(1, 16)]\n",
    "    predictions = df_pred[x_pred_cols + y_pred_cols].copy()\n",
    "    \n",
    "    print(f\"\\n✓ Prediction complete! Shape: {predictions.shape}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # EVALUATE IF GROUND TRUTH PROVIDED\n",
    "    # ========================================\n",
    "    metrics = {}\n",
    "    \n",
    "    if df_true_x is not None and df_true_y is not None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION METRICS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Calculate metrics for each frame\n",
    "        mae_per_frame_x = []\n",
    "        mae_per_frame_y = []\n",
    "        rmse_per_frame_x = []\n",
    "        rmse_per_frame_y = []\n",
    "        \n",
    "        for i in range(1, 16):\n",
    "            x_col = f'x_{i}'\n",
    "            y_col = f'y_{i}'\n",
    "            \n",
    "            # Filter out NaN values (for players without all frames)\n",
    "            valid_mask_x = df_true_x[x_col].notna()\n",
    "            valid_mask_y = df_true_y[y_col].notna()\n",
    "            \n",
    "            if valid_mask_x.sum() > 0:\n",
    "                mae_x = mean_absolute_error(df_true_x.loc[valid_mask_x, x_col], \n",
    "                                            predictions.loc[valid_mask_x, x_col])\n",
    "                rmse_x = np.sqrt(mean_squared_error(df_true_x.loc[valid_mask_x, x_col], \n",
    "                                                     predictions.loc[valid_mask_x, x_col]))\n",
    "                mae_per_frame_x.append(mae_x)\n",
    "                rmse_per_frame_x.append(rmse_x)\n",
    "            else:\n",
    "                mae_per_frame_x.append(np.nan)\n",
    "                rmse_per_frame_x.append(np.nan)\n",
    "            \n",
    "            if valid_mask_y.sum() > 0:\n",
    "                mae_y = mean_absolute_error(df_true_y.loc[valid_mask_y, y_col], \n",
    "                                            predictions.loc[valid_mask_y, y_col])\n",
    "                rmse_y = np.sqrt(mean_squared_error(df_true_y.loc[valid_mask_y, y_col], \n",
    "                                                     predictions.loc[valid_mask_y, y_col]))\n",
    "                mae_per_frame_y.append(mae_y)\n",
    "                rmse_per_frame_y.append(rmse_y)\n",
    "            else:\n",
    "                mae_per_frame_y.append(np.nan)\n",
    "                rmse_per_frame_y.append(np.nan)\n",
    "        \n",
    "        metrics['mae_per_frame_x'] = mae_per_frame_x\n",
    "        metrics['mae_per_frame_y'] = mae_per_frame_y\n",
    "        metrics['rmse_per_frame_x'] = rmse_per_frame_x\n",
    "        metrics['rmse_per_frame_y'] = rmse_per_frame_y\n",
    "        metrics['overall_mae_x'] = np.nanmean(mae_per_frame_x)\n",
    "        metrics['overall_mae_y'] = np.nanmean(mae_per_frame_y)\n",
    "        metrics['overall_rmse_x'] = np.nanmean(rmse_per_frame_x)\n",
    "        metrics['overall_rmse_y'] = np.nanmean(rmse_per_frame_y)\n",
    "        \n",
    "        print(f\"\\nOverall X MAE: {metrics['overall_mae_x']:.4f} yards\")\n",
    "        print(f\"Overall Y MAE: {metrics['overall_mae_y']:.4f} yards\")\n",
    "        print(f\"Overall X RMSE: {metrics['overall_rmse_x']:.4f} yards\")\n",
    "        print(f\"Overall Y RMSE: {metrics['overall_rmse_y']:.4f} yards\")\n",
    "        \n",
    "        print(f\"\\nPer-frame X MAE:\")\n",
    "        for i, mae in enumerate(mae_per_frame_x, 1):\n",
    "            print(f\"  Frame {i:2d}: {mae:.4f} yards\")\n",
    "        \n",
    "        print(f\"\\nPer-frame Y MAE:\")\n",
    "        for i, mae in enumerate(mae_per_frame_y, 1):\n",
    "            print(f\"  Frame {i:2d}: {mae:.4f} yards\")\n",
    "    \n",
    "    # ========================================\n",
    "    # VISUALIZATION\n",
    "    # ========================================\n",
    "    if plot_results and df_true_x is not None and df_true_y is not None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"GENERATING VISUALIZATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot 1: MAE per frame\n",
    "        ax1 = axes[0, 0]\n",
    "        frames = list(range(1, 16))\n",
    "        ax1.plot(frames, mae_per_frame_x, 'o-', label='X MAE', linewidth=2, markersize=8)\n",
    "        ax1.plot(frames, mae_per_frame_y, 's-', label='Y MAE', linewidth=2, markersize=8)\n",
    "        ax1.set_xlabel('Future Frame', fontsize=12)\n",
    "        ax1.set_ylabel('MAE (yards)', fontsize=12)\n",
    "        ax1.set_title('Mean Absolute Error per Future Frame', fontsize=14, fontweight='bold')\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: RMSE per frame\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.plot(frames, rmse_per_frame_x, 'o-', label='X RMSE', linewidth=2, markersize=8)\n",
    "        ax2.plot(frames, rmse_per_frame_y, 's-', label='Y RMSE', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Future Frame', fontsize=12)\n",
    "        ax2.set_ylabel('RMSE (yards)', fontsize=12)\n",
    "        ax2.set_title('Root Mean Squared Error per Future Frame', fontsize=14, fontweight='bold')\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Sample trajectories (actual vs predicted)\n",
    "        ax3 = axes[1, 0]\n",
    "        for idx in range(min(sample_trajectories, len(df_input))):\n",
    "            # Actual trajectory\n",
    "            x_true = [df_input.iloc[idx]['x_0']] + [df_true_x.iloc[idx][f'x_{i}'] for i in range(1, 16) if pd.notna(df_true_x.iloc[idx][f'x_{i}'])]\n",
    "            y_true = [df_input.iloc[idx]['y_0']] + [df_true_y.iloc[idx][f'y_{i}'] for i in range(1, 16) if pd.notna(df_true_y.iloc[idx][f'y_{i}'])]\n",
    "            \n",
    "            # Predicted trajectory\n",
    "            x_pred = [df_input.iloc[idx]['x_0']] + [predictions.iloc[idx][f'x_{i}'] for i in range(1, len(x_true))]\n",
    "            y_pred = [df_input.iloc[idx]['y_0']] + [predictions.iloc[idx][f'y_{i}'] for i in range(1, len(y_true))]\n",
    "            \n",
    "            ax3.plot(x_true, y_true, 'o-', label=f'Player {idx+1} (actual)', alpha=0.7)\n",
    "            ax3.plot(x_pred, y_pred, 's--', label=f'Player {idx+1} (predicted)', alpha=0.7)\n",
    "        \n",
    "        ax3.set_xlabel('X Position (yards)', fontsize=12)\n",
    "        ax3.set_ylabel('Y Position (yards)', fontsize=12)\n",
    "        ax3.set_title(f'Sample Trajectories: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "        ax3.legend(fontsize=9, ncol=2)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Error distribution (combined x and y)\n",
    "        ax4 = axes[1, 1]\n",
    "        all_errors = []\n",
    "        for i in range(1, 16):\n",
    "            valid_mask_x = df_true_x[f'x_{i}'].notna()\n",
    "            valid_mask_y = df_true_y[f'y_{i}'].notna()\n",
    "            \n",
    "            errors_x = np.abs(df_true_x.loc[valid_mask_x, f'x_{i}'] - predictions.loc[valid_mask_x, f'x_{i}'])\n",
    "            errors_y = np.abs(df_true_y.loc[valid_mask_y, f'y_{i}'] - predictions.loc[valid_mask_y, f'y_{i}'])\n",
    "            \n",
    "            all_errors.extend(errors_x.tolist())\n",
    "            all_errors.extend(errors_y.tolist())\n",
    "        \n",
    "        ax4.hist(all_errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax4.set_xlabel('Absolute Error (yards)', fontsize=12)\n",
    "        ax4.set_ylabel('Frequency', fontsize=12)\n",
    "        ax4.set_title('Error Distribution (All Frames, X & Y Combined)', fontsize=14, fontweight='bold')\n",
    "        ax4.axvline(np.mean(all_errors), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(all_errors):.2f}')\n",
    "        ax4.legend(fontsize=11)\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✓ Visualizations complete\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAJECTORY PREDICTION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return predictions, metrics\n",
    "\n",
    "print(\"\\n✓ Function 'predict_full_trajectory()' created and ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prediction function on the test dataset\n",
    "print(\"Testing the sequential prediction function on test data...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the test datasets we prepared earlier\n",
    "# df_test_X: input features (no output frames)\n",
    "# df_test_y_true: ground truth output frames (x_1:x_15, y_1:y_15)\n",
    "\n",
    "# Split ground truth into x and y components\n",
    "x_true_cols = [f'x_{i}' for i in range(1, 16)]\n",
    "y_true_cols = [f'y_{i}' for i in range(1, 16)]\n",
    "\n",
    "df_test_y_true_x = df_test_y_true[x_true_cols]\n",
    "df_test_y_true_y = df_test_y_true[y_true_cols]\n",
    "\n",
    "# Make predictions with evaluation\n",
    "test_predictions, test_metrics = predict_full_trajectory(\n",
    "    df_input=df_test_X,\n",
    "    df_true_x=df_test_y_true_x,\n",
    "    df_true_y=df_test_y_true_y,\n",
    "    plot_results=True,\n",
    "    sample_trajectories=5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test samples: {len(test_predictions)}\")\n",
    "print(f\"Overall X MAE: {test_metrics['overall_mae_x']:.4f} yards\")\n",
    "print(f\"Overall Y MAE: {test_metrics['overall_mae_y']:.4f} yards\")\n",
    "print(f\"Overall X RMSE: {test_metrics['overall_rmse_x']:.4f} yards\")\n",
    "print(f\"Overall Y RMSE: {test_metrics['overall_rmse_y']:.4f} yards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prediction function on the test dataset\n",
    "print(\"Testing the sequential prediction function on test data...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the test datasets we prepared earlier\n",
    "# df_test_X: input features (no output frames)\n",
    "# df_test_y_true: ground truth output frames (x_1:x_15, y_1:y_15)\n",
    "\n",
    "# Split ground truth into x and y components\n",
    "x_true_cols = [f'x_{i}' for i in range(1, 16)]\n",
    "y_true_cols = [f'y_{i}' for i in range(1, 16)]\n",
    "\n",
    "df_test_y_true_x = df_test_y_true[x_true_cols]\n",
    "df_test_y_true_y = df_test_y_true[y_true_cols]\n",
    "\n",
    "# Make predictions with evaluation\n",
    "test_predictions, test_metrics = predict_full_trajectory(\n",
    "    df_input=df_test_X,\n",
    "    df_true_x=df_test_y_true_x,\n",
    "    df_true_y=df_test_y_true_y,\n",
    "    plot_results=True,\n",
    "    sample_trajectories=5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test samples: {len(test_predictions)}\")\n",
    "print(f\"Overall X MAE: {test_metrics['overall_mae_x']:.4f} yards\")\n",
    "print(f\"Overall Y MAE: {test_metrics['overall_mae_y']:.4f} yards\")\n",
    "print(f\"Overall X RMSE: {test_metrics['overall_rmse_x']:.4f} yards\")\n",
    "print(f\"Overall Y RMSE: {test_metrics['overall_rmse_y']:.4f} yards\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uliana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
